# Timeout value if resources are not available in your
# availability zone. This default is a "short" 3 minutes

capacity_reservation_create_timeout  = "3m"

# Expiration time (in minutes) for capacity reservations.

capacity_reservation_expiration      = "10m"

# Which components to deploy
#
# Valid answers are "clients", "storage", "hammerspace", "ecgroup", or "all".
# The structure is a list, so you can say "storage", "hammerspace" to deploy
# both of those

deploy_components		     = ["all"]

# If you do not have permissions in your AWS environment to create roles
# and permissions, then enter the name of a predefined role with appropriate
# permissions in the iam_profile_name variable. If you have the capability
# to create a role, then comment out that variable

# iam_profile_name	             = "Hammerspace"

# Store the public and private key for Ansible

ansible_private_key_secret_arn       = "arn:aws:secretsmanager:us-east-1:919243021404:secret:ansible-controller-private-key-EXAMPLE"
ansible_ssh_public_key               = "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIIe/LNfPiKvDPeNIIJmKO/JEdzNmPowIZ9L2EXAMPLE Ansible Controller Key"

# Custom AMI owner ID's
# This is a list of the owners of any custom AMI's. Since Hammerspace is a custom AMI,
# in order to provide guard-rail checking if the AMI exists in a given region, we will
# need to specify the AMI owner. In this case, Hammerspace and Ubuntu owners...

custom_ami_owner_ids  	  	     = ["919243021404", "679593333241"]

# Placement Group
#
# Comment out the following two variables if you don't want your resources
# placed into a placement group. 

placement_group_name		     = "Terraform-Group"
placement_group_strategy	     = "cluster"

# Structure to configure volume groups, shares within the Anvil. This is passed
# to Ansible and the ansible playbook will add the necessary services
#
# If you wish to not configure the anvil for storage volumes, etc, then just make
# the "config_ansible = null"

config_ansible = {
  allow_root = true
  ecgroup_volume_group = "xyz"
  ecgroup_share_name = "123"
  volume_groups = {
    group_1 = {
      volumes = ["1","2","3","4"]
      share   = "group_1_share"
    }
    group_2 = {
      add_groups = ["group_1"]
      volumes    = ["5","6","7","8"]
      share      = "group_2_share"
    }
    group_3 = {
      add_groups = ["group_1", "group_2"]
      volumes    = ["9","10","11","12"]
      share      = "group_3_share"
    }
  }
}

# Global variables
#
# Change your project_name, the key_name, and the vpc and subnet id's

project_name		             = "AWS-Project"
region				     = "us-west-2"
key_name   			     = "Some-PEM-Key"

# There are now two ways to use VPC / Subnet(s). You can either use existing
# or create your own.

# If you wish to use your own, then uncomment these and fill them in

# vpc_id     			     = "vpc-e3b5890"
# subnet_id  			     = "subnet-0d8a5034456df"
# public_subnet_id		     = "subnet-somepublic"

# If you wish to create your own VPC / Subnet(s), then uncomment these
# and fill in the cidr for the vpc / subnet(s). Even if you don't use the
# public subnet(s), currently YOU MUST create them. Just set the variable
# "assign_public_ip" to false
#
# Finally, this will create subnet(s) in two availability zones. This is
# also REQUIRED in this version even if you only use one of them. In fact,
# the default will be to utilize private_subnet_1 for almost everything
# (with some minor differences)

vpc_cidr				= "10.10.0.0/16"
private_subnet_1_cidr			= "10.10.1.0/24"
private_subnet_2_cidr			= "10.10.2.0/24"
public_subnet_1_cidr			= "10.10.3.0/24"
public_subnet_2_cidr			= "10.10.4.0/24"
subnet_1_az				= "us-east-1a"
subnet_2_az				= "us-east-1b"

# Set this variable is you want public ip addresses for each of the instances.
# Otherwise, only private ip addresses will be assigned.
#
# You must ALSO use a separate public subnet that routes to the internet. Just
# assigning a public IP doesn't always work unless you are very good with
# setting up a NAT gateway...

assign_public_ip  	     	     = false

# This is the allowed ingress cidr blocks. These will be added to the vpc cidr automatically
# upon instantiation.

allowed_source_cidr_blocks	     = ["10.0.0.0/16", "192.168.254.0/24"]

# You can put in as many tags as you would like. The format should be self-explanatory

tags = {
  Name		= "Example Project Name" # AWS Sizing (example)
  Owner		= "Example Owner" # Your name
  Environment	= "Example Environment" # development, qa, staging, production, sandbox
  CostCenter	= "Sales" # Name of a cost center
}

# ---------------------------------------------------------------
# Module specific variables
# ---------------------------------------------------------------

# Ansible specific variables (ansible_ prefix)
#
# So, the ansible_target_user is the OS_NAME for the login. Once you
# have logged in once, you can create as many users as you want. But,
# the first default user is the OS_NAME (or in this case "ubuntu")
#

ansible_ami			     = "ami-04cfeb9ad57f3053a"
ansible_instance_type		     = "m5n.2xlarge"
ansible_boot_volume_size	     = 40
ansible_boot_volume_type	     = "gp2"

# Client specific variables (clients_ prefix)
#
# So, the clients_target_user is the OS_NAME for the login. Once you
# have logged in once, you can create as many users as you want. But,
# that first default user is the OS_NAME (or in this case "ubuntu"

clients_instance_count		     = 2
clients_ami 			     = "ami-04cfeb9ad57f3053a"
clients_instance_type 		     = "m5n.2xlarge"
clients_tier0			     = false
clients_tier0_type		     = "raid-0"
clients_boot_volume_size	     = 100
clients_boot_volume_type	     = "gp2"
clients_ebs_count 		     = 0
clients_ebs_size  		     = 1000
clients_ebs_type  		     = "gp3"
clients_ebs_iops  		     = 9000
clients_ebs_throughput	 	     = 1000

# An ECGroup is a cluster of Linux Storage Servers that have combined
# to pool their storage into an erasure coded array. Since any ecgroup
# is composed of multiple linux storage servers, you will need a minimum
# of 4 nodes and a maximum of 16 nodes to form an ECGroup cluster.

ecgroup_instance_type               = "m5n.2xlarge"
ecgroup_node_count                  = 4
ecgroup_metadata_volume_type        = "gp3"
ecgroup_metadata_volume_size        = 4096
ecgroup_metadata_volume_throughput  = 1000
ecgroup_metadata_volume_iops        = 9000
ecgroup_storage_volume_count        = 4
ecgroup_storage_volume_type         = "gp3"
ecgroup_storage_volume_size         = 1024
ecgroup_storage_volume_throughput   = 1000
ecgroup_storage_volume_iops         = 9000

# These are the variable to deploy a Hammerspace Anvil and
# zero or more DSX storage servers.

# Both the Anvil and the DSX share an AMI. So, we only hae
# to declare it once.

hammerspace_ami			     = "ami-094d8e62982f34834"

# Some environments have a specific security group requirement.
# Since it is impossible for us to know what you would like, we
# have given you the option of specifying it yourself for both
# the Anvil and separately for the DSX. If you don't specify it
# we will create a generic Security Group

# --- Optional: Provide existing Security Group IDs 
# hammerspace_anvil_security_group_id  = "sg-0f888587d7e83bda2"
# hammerspace_dsx_security_group_id    = "sg-0f888587d7e83bda2"

# Once you deploy a standalone Anvil, it is hard to change that to a HA pair. The
# anvil_destruction variable that follows prevents a user from changing the anvil_count
# and trying to add another anvil to an already existing standalone anvil. Set this variable
# to true if you wish to destroy the anvil on a terraform destroy.

hammerspace_sa_anvil_destruction     = true

# Anvil specific

hammerspace_anvil_count		     = 2
hammerspace_anvil_instance_type      = "m5n.2xlarge"
hammerspace_anvil_meta_disk_size     = 1000
hammerspace_anvil_meta_disk_type     = "gp3"
hammerspace_anvil_meta_disk_iops     = 9000
hammerspace_anvil_meta_disk_throughput = 1000

# DSX specific

hammerspace_dsx_count                = 1
hammerspace_dsx_instance_type 	     = "m5n.2xlarge"
hammerspace_dsx_ebs_count	     = 5
hammerspace_dsx_ebs_size 	     = 1000
hammerspace_dsx_ebs_type	     = "gp3"
hammerspace_dsx_ebs_iops	     = 6000
hammerspace_dsx_ebs_throughput	     = 1000

# Here is where you can configure Linux Storage Servers.
# Notice that you can configure raid-0, raid-5, or raid-6.
# You must either use an instance type that has internal
# nvme drives or add the appropriate number of storage_ebs_count
# in order to form the raid array on each storage server.

storage_instance_count	 	     = 5
storage_ami 			     = "ami-04cfeb9ad57f3053a"
storage_instance_type 		     = "m5n.2xlarge"
storage_boot_volume_size	     = 100
storage_boot_volume_type	     = "gp2"
storage_raid_level		     = "raid-6"
storage_ebs_count 		     = 6
storage_ebs_size  		     = 1000
storage_ebs_type  		     = "gp3"
storage_ebs_iops  		     = 9000
storage_ebs_throughput	 	     = 1000

# Amazon MQ Service

hosted_zone_name		     = "houston.internal" # DNS needed for Amazon MQ
amazonmq_engine_version		     = "4.2"
amazonmq_instance_type		     = "mq.m7g.large"
amazonmq_admin_username		     = "admin"
amazonmq_admin_password		     = "Pl3as3F1ll1n"
amazonmq_site_admin_username	     = "admin"
amazonmq_site_admin_password	     = "Pl3as3F1ll1n"
amazonmq_site_admin_password_hash    = "This is a hash created by rabbitmq"