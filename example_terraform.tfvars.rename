# Timeout value if resources are not available in your
# availability zone. This default is a "short" 3 minutes

capacity_reservation_create_timeout  = "3m"

# Which components to deploy
#
# Valid answers are "clients", "storage", "hammerspace", "ecgroup", or "all".
# The structure is a list, so you can say "storage", "hammerspace" to deploy
# both of those

deploy_components		     = ["all"]

# Set this variable is you want public ip addresses for each of the instances.
# Otherwise, only private ip addresses will be assigned.
#
# You must ALSO use a separate public subnet that routes to the internet. Just
# assigning a public IP doesn't always work unless you are very good with
# setting up a NAT gateway...

assign_public_ip  	     	     = false
# public_subnet_id		     = "subnet-somepublic"

# If you do not have permissions in your AWS environment to create roles
# and permissions, then enter the name of a predefined role with appropriate
# permissions in the iam_profile_name variable. If you have the capability
# to create a role, then comment out that variable

# iam_profile_name	             = "Hammerspace"

# Custom AMI owner ID's
# This is a list of the owners of any custom AMI's. Since Hammerspace is a custom AMI,
# in order to provide guard-rail checking if the AMI exists in a given region, we will
# need to specify the AMI owner. In this case, Hammerspace and Ubuntu owners...

custom_ami_owner_ids  	  	     = ["919243021404", "679593333241"]

# Placement Group
#
# Comment out the following two variables if you don't want your resources
# placed into a placement group. 

placement_group_name		     = "Terraform-Group"
placement_group_strategy	     = "cluster"

# Global variables
#
# Change your project_name, the key_name, and the vpc and subnet id's

project_name		             = "AWS-Project"
key_name   			     = "Some-PEM-Key"
vpc_id     			     = "vpc-e3b5890"
subnet_id  			     = "subnet-0d8a5034456df"
region				     = "us-west-2"
allow_root			     = false

# This is the allowed ingress cidr blocks. These will be added to the vpc cidr automatically
# upon instantiation.

allowed_source_cidr_blocks	     = ["10.0.0.0/16", "192.168.254.0/24"]

# You can put in as many tags as you would like. The format should be self-explanatory

tags = {
  Name		= "Example Project Name" # AWS Sizing (example)
  Owner		= "Example Owner" # Your name
  Environment	= "Example Environment" # development, qa, staging, production, sandbox
  CostCenter	= "Sales" # Name of a cost center
}

# ---------------------------------------------------------------
# Module specific variables
# ---------------------------------------------------------------

# Ansible specific variables (ansible_ prefix)
#
# So, the ansible_target_user is the OS_NAME for the login. Once you
# have logged in once, you can create as many users as you want. But,
# the first default user is the OS_NAME (or in this case "ubuntu")
#

ansible_ami			     = "ami-04cfeb9ad57f3053a"
ansible_instance_type		     = "m5n.2xlarge"
ansible_boot_volume_size	     = 40
ansible_boot_volume_type	     = "gp2"

# Client specific variables (clients_ prefix)
#
# So, the clients_target_user is the OS_NAME for the login. Once you
# have logged in once, you can create as many users as you want. But,
# that first default user is the OS_NAME (or in this case "ubuntu"

clients_instance_count		     = 2
clients_ami 			     = "ami-04cfeb9ad57f3053a"
clients_instance_type 		     = "m5n.2xlarge"
clients_tier0			     = false
clients_tier0_type		     = "raid-0"
clients_boot_volume_size	     = 100
clients_boot_volume_type	     = "gp2"
clients_ebs_count 		     = 0
clients_ebs_size  		     = 1000
clients_ebs_type  		     = "gp3"
clients_ebs_iops  		     = 9000
clients_ebs_throughput	 	     = 1000

# An ECGroup is a cluster of Linux Storage Servers that have combined
# to pool their storage into an erasure coded array. Since any ecgroup
# is composed of multiple linux storage servers, you will need a minimum
# of 4 nodes and a maximum of 16 nodes to form an ECGroup cluster.

ecgroup_instance_type               = "m5n.2xlarge"
ecgroup_node_count                  = 4
ecgroup_metadata_volume_type        = "gp3"
ecgroup_metadata_volume_size        = 4096
ecgroup_metadata_volume_throughput  = 1000
ecgroup_metadata_volume_iops        = 9000
ecgroup_storage_volume_count        = 4
ecgroup_storage_volume_type         = "gp3"
ecgroup_storage_volume_size         = 1024
ecgroup_storage_volume_throughput   = 1000
ecgroup_storage_volume_iops         = 9000

# These are the variable to deploy a Hammerspace Anvil and
# zero or more DSX storage servers.

# Both the Anvil and the DSX share an AMI. So, we only hae
# to declare it once.

hammerspace_ami			     = "ami-094d8e62982f34834"

# Some environments have a specific security group requirement.
# Since it is impossible for us to know what you would like, we
# have given you the option of specifying it yourself for both
# the Anvil and separately for the DSX. If you don't specify it
# we will create a generic Security Group

# --- Optional: Provide existing Security Group IDs 
# hammerspace_anvil_security_group_id  = "sg-0f888587d7e83bda2"
# hammerspace_dsx_security_group_id    = "sg-0f888587d7e83bda2"

# Once you deploy a standalone Anvil, it is hard to change that to a HA pair. The
# anvil_destruction variable that follows prevents a user from changing the anvil_count
# and trying to add another anvil to an already existing standalone anvil. Set this variable
# to true if you wish to destroy the anvil on a terraform destroy.

hammerspace_sa_anvil_destruction     = true

# Anvil specific

hammerspace_anvil_count		     = 2
hammerspace_anvil_instance_type      = "m5n.2xlarge"
hammerspace_anvil_meta_disk_size     = 1000
hammerspace_anvil_meta_disk_type     = "gp3"
hammerspace_anvil_meta_disk_iops     = 9000
hammerspace_anvil_meta_disk_throughput = 1000

# DSX specific

hammerspace_dsx_count                = 1
hammerspace_dsx_instance_type 	     = "m5n.2xlarge"
hammerspace_dsx_ebs_count	     = 5
hammerspace_dsx_ebs_size 	     = 1000
hammerspace_dsx_ebs_type	     = "gp3"
hammerspace_dsx_ebs_iops	     = 6000
hammerspace_dsx_ebs_throughput	     = 1000

# Here is where you can configure Linux Storage Servers.
# Notice that you can configure raid-0, raid-5, or raid-6.
# You must either use an instance type that has internal
# nvme drives or add the appropriate number of storage_ebs_count
# in order to form the raid array on each storage server.

storage_instance_count	 	     = 5
storage_ami 			     = "ami-04cfeb9ad57f3053a"
storage_instance_type 		     = "m5n.2xlarge"
storage_boot_volume_size	     = 100
storage_boot_volume_type	     = "gp2"
storage_raid_level		     = "raid-6"
storage_ebs_count 		     = 6
storage_ebs_size  		     = 1000
storage_ebs_type  		     = "gp3"
storage_ebs_iops  		     = 9000
storage_ebs_throughput	 	     = 1000

