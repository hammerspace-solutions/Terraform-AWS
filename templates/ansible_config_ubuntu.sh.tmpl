#!/bin/bash

# Variable placeholders - replaced by Terraform templatefile function

TARGET_NODES_JSON='${TARGET_NODES_JSON}'
MGMT_IP='${MGMT_IP}'
ANVIL_ID='${ANVIL_ID}'
BASTION_INSTANCES='${BASTION_INSTANCES}'
CLIENT_INSTANCES='${CLIENT_INSTANCES}'
STORAGE_INSTANCES='${STORAGE_INSTANCES}'
VG_NAME='${VG_NAME}'
SHARE_NAME='${SHARE_NAME}'
ECGROUP_INSTANCES='${ECGROUP_INSTANCES}'
ECGROUP_HOSTS='${ECGROUP_HOSTS}'
ECGROUP_NODES='${ECGROUP_NODES}'
ECGROUP_METADATA_ARRAY='${ECGROUP_METADATA_ARRAY}'
ECGROUP_STORAGE_ARRAY='${ECGROUP_STORAGE_ARRAY}'
TARGET_USER='${TARGET_USER}'
TARGET_HOME='${TARGET_HOME}'
SSH_KEYS='${SSH_KEYS}'
ALLOW_ROOT='${ALLOW_ROOT}'

# Some private variables for dealing with ssh-less logins using root

ROOT_USER='${ROOT_USER}'
ROOT_HOME='${ROOT_HOME}'
PRIVATE_KEY_FILE='${PRIVATE_KEY_FILE}'
PRIVATE_KEY_PATH='${PRIVATE_KEY_PATH}'
ROOT_PRIVATE_KEY_PATH='${ROOT_PRIVATE_KEY_PATH}'

# --- Script ---

set -euo pipefail

# --- Package Installation ---

sudo apt-get -y update
sudo apt-get install -y software-properties-common
sudo add-apt-repository --yes --update ppa:ansible/ansible
sudo apt-get install -y ansible jq net-tools

# --- Install Emacs (because I like it) ---

sudo DEBIAN_FRONTEND=noninteractive apt install -y emacs

echo "Upgrade the OS to make sure we have the latest"
sudo apt-get -y upgrade

# Get rid of fingerprint checking on ssh
# We need this in case somebody wants to run automated scripts. Otherwise,
# they will have to modify their scripts to answer the stupid question of
# "are you sure"?

sudo tee -a /etc/ssh/ssh_config > /dev/null <<'EOF'
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
EOF
sudo tee -a /etc/ssh/sshd_config > /dev/null <<'EOF'
    PermitRootLogin yes
    PubkeyAuthentication yes
EOF

# Restart the sshd

systemctl restart ssh

# --- SSH Key Management for additional keys ---

if [ -n "$${SSH_KEYS}" ]; then
    echo "Starting SSH Key Management Deployment"

    if [ ! -d "$${TARGET_HOME}/.ssh" ]; then
      mkdir -p "$${TARGET_HOME}/.ssh"
      chmod 700 "$${TARGET_HOME}/.ssh"
      touch "$${TARGET_HOME}/.ssh/authorized_keys"
    fi
    
    # Process keys one by one to avoid multi-line issues

    echo "$${SSH_KEYS}" | while read -r key; do
        if [ -n "$${key}" ] && ! grep -qF "$${key}" "$${TARGET_HOME}/.ssh/authorized_keys"; then
            echo "$${key}" >> "$${TARGET_HOME}/.ssh/authorized_keys"
        fi
    done

    chmod 600 "$${TARGET_HOME}/.ssh/authorized_keys"
    chown -R "$${TARGET_USER}:$${TARGET_USER}" "$${TARGET_HOME}/.ssh"
    echo "Ending SSH Key Management Deployment"
fi

# --- SSH Key Management for additional keys (This is ONLY for root) ---

if [ -n "$${SSH_KEYS}" ]; then
    echo "Starting SSH Key Management Deployment for root"

    if [ ! -d "$${ROOT_HOME}/.ssh" ]; then
      mkdir -p "$${ROOT_HOME}/.ssh"
      chmod 700 "$${ROOT_HOME}/.ssh"
      touch "$${ROOT_HOME}/.ssh/authorized_keys"
    fi
    
    # Process keys line by line
    
    echo "$${SSH_KEYS}" | while read -r key; do
        if [ -n "$${key}" ] && ! grep -qF "$${key}" "$${ROOT_HOME}/.ssh/authorized_keys"; then
            echo "$${key}" >> "$${ROOT_HOME}/.ssh/authorized_keys"
        fi
    done

    chmod 600 "$${ROOT_HOME}/.ssh/authorized_keys"
    chown -R "$${ROOT_USER}":"$${ROOT_USER}" "$${ROOT_HOME}/.ssh"
    echo "Ending SSH Key Management Deployment for $${ROOT_USER}"
fi

# Wait for the Terraform provisioner to copy the admin private key.
# This loop prevents the script from running Ansible commands before the
# key is available, resolving the race condition.

echo "Waiting for Ansible private key to be provisioned at $${PRIVATE_KEY_PATH}..."
SECONDS_WAITED=0
while [ ! -f "$${PRIVATE_KEY_PATH}" ]; do
    if [ "$${SECONDS_WAITED}" -gt 1200 ]; then
        echo "ERROR: Timed out after 20 minutes waiting for private key." >&2
        exit 1
    fi
    sleep 5
    SECONDS_WAITED=$((SECONDS_WAITED + 5))
    echo "Still waiting for key..."
done
echo "Ansible private key found. Proceeding with configuration."

# Copy the private key to the root .ssh

cp "$${PRIVATE_KEY_PATH}" "$${ROOT_PRIVATE_KEY_PATH}"
chown root:root "${ROOT_PRIVATE_KEY_PATH}"
chmod 600 "${ROOT_PRIVATE_KEY_PATH}"

# --- Passwordless SSH Setup (for bastion client, clients, and storage) ---

# We are having problems where some of the clients, storage, and whatnot are
# slow in coming available. Make sure we delay here so that they have time
# to become available. THIS IS NOT a permanent fix. That is forthcoming.

echo "SLEEP while waiting for clients, storage to become available"
sleep 180
echo "End SLEEP"

# This is going to be done in three parts:
#
# Part 1: Create the inventory.ini file

if [ -n "$${TARGET_NODES_JSON}" ] && [ "$${TARGET_NODES_JSON}" != "[]" ]; then
    echo "Setting up for passwordless SSH... Creating inventory.ini file"
    sudo -u ${TARGET_USER} ansible-galaxy collection install community.crypto

    INVENTORY_NAME="inventory.ini"
    INVENTORY_FILE="$${TARGET_HOME}/$INVENTORY_NAME"
    echo "[all:children]" > $INVENTORY_FILE

    # Extract unique types and append to [all]
    echo "$${TARGET_NODES_JSON}" | jq -r '.[].type' | sort -u >> $INVENTORY_FILE

    # Output each type group with private IPs
    for type in bastion client storage anvil dsx; do
	HOSTS="$(echo "$${TARGET_NODES_JSON}" | jq -r --arg t $type '.[] | select(.type == $t) | .private_ip')"
	if [ -n "$${HOSTS}" ]; then
	    echo "" >> $INVENTORY_FILE
	    echo "[$type]" >> $INVENTORY_FILE
	    echo "$${HOSTS}" >> $INVENTORY_FILE
	fi
    done

    echo "" >> $INVENTORY_FILE
    echo "[ssh:children]" >> $INVENTORY_FILE
    for type in bastion client storage; do
	if echo "$${TARGET_NODES_JSON}" | jq -e --arg t $type '.[] | select(.type == $t)' > /dev/null; then
	    echo $type >> $INVENTORY_FILE
	fi
    done

    echo "" >> $INVENTORY_FILE
    echo "[hammerspace:children]" >> $INVENTORY_FILE
    for type in anvil dsx; do
	if echo "$${TARGET_NODES_JSON}" | jq -e --arg t $type '.[] | select(.type == $t)' > /dev/null; then
	    echo $type >> $INVENTORY_FILE
	fi
    done

    chown ${TARGET_USER}:${TARGET_USER} $INVENTORY_FILE

    # Copy this file to the root directory. This MIGHT be needed if we
    # distribute keys for root as well.

    cp $INVENTORY_FILE "$${ROOT_HOME}"
    chown ${ROOT_USER}:${ROOT_USER} "$${ROOT_HOME}/$INVENTORY_NAME"

    # Copy this file to the bastion host (if it started)

    if [ -n "$${BASTION_INSTANCES}" ] && [ "$${BASTION_INSTANCES}" != "[]" ]; then

      # Get the bastion IP

      bastion_host=$(ansible-inventory -i $INVENTORY_FILE --list | jq -r --arg grp bastion '.[$grp].hosts[]')
      echo "The bastion host is $bastion_host"
      scp $INVENTORY_FILE $bastion_host:"$${TARGET_HOME}"
    fi
else
    echo "No clients or storage servers deployed, skipping creating inventory.ini file"
fi

# Part 2: Create and execute the ansible playbook and update the known_hosts

if [ -n "$${TARGET_NODES_JSON}" ] && [ "$${TARGET_NODES_JSON}" != "[]" ]; then

    PLAYBOOK_FILE="$${TARGET_HOME}/distribute_keys.yml"
    cat > "$PLAYBOOK_FILE" << EOF
---
- name: Distribute shared SSH key pair and configure known_hosts
  hosts: ssh
  become: true
  vars:
    ssh_user: "ubuntu"
    ssh_dir: "/home/{{ ssh_user }}/.ssh"
    private_key_source: "{{ ssh_dir }}/id_rsa"
    public_key_source: "{{ ssh_dir }}/id_rsa.pub"
    known_hosts_file: "{{ ssh_dir }}/known_hosts"

  tasks:
    - name: Ensure .ssh directory exists
      file:
        path: "{{ ssh_dir }}"
        state: directory
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
        mode: '0700'

    - name: Copy shared private key
      copy:
        src: "{{ private_key_source }}"
        dest: "{{ ssh_dir }}/id_rsa"
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
        mode: '0600'

    - name: Copy shared public key
      copy:
        src: "{{ public_key_source }}"
        dest: "{{ ssh_dir }}/id_rsa.pub"
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
        mode: '0644'

    - name: Add public key to authorized_keys
      authorized_key:
        user: "{{ ssh_user }}"
        key: "{{ lookup('file', public_key_source) }}"
        state: present
EOF
  
    chown ${TARGET_USER}:${TARGET_USER} "$PLAYBOOK_FILE"

    # Get all the IPs for names under the group [ssh]
    TARGET_GROUP="ssh"
    ALL_SSH_IPS=$(
        ansible-inventory -i "$INVENTORY_FILE" --list |
  	  jq -r --arg grp "$TARGET_GROUP" '
	    .[$grp].children[] as $child |
	    .[$child].hosts[]?'
    )

    # Do a ssh-keyscan and send it to every host to update known_hosts
    if [ -n "$ALL_SSH_IPS" ]; then
      echo "Scanning hosts to populate known_hosts..."
      echo "$ALL_SSH_IPS" | awk 'NF' | sort -u | xargs -r ssh-keyscan -H >> "$${TARGET_HOME}/.ssh/known_hosts"
    fi

    echo "Running Ansible playbook to distribute SSH keys..."
    sudo -u ${TARGET_USER} bash -c "ansible-playbook -i $INVENTORY_FILE ${TARGET_HOME}/distribute_keys.yml"
    echo "Finished distributing SSH keys."
  else
    echo "No clients or storage servers deployed, skipping passwordless SSH setup."
fi

# Part-3: Distribute keys for root users (only if they want them)

if [ -n "$${TARGET_NODES_JSON}" ] && [ "$${TARGET_NODES_JSON}" != "[]" ] &&
   [ "$${ALLOW_ROOT}" = "true" ]; then
    echo "Setting up for passwordless SSH for root..."

    PLAYBOOK_FILE="$${ROOT_HOME}/distribute_keys.yml"
    cat > "$PLAYBOOK_FILE" << EOF
---
- name: Distribute shared SSH key pair and configure known_hosts
  hosts: ssh
  become: true
  vars:
    ssh_user: "root"
    ssh_dir: "/{{ ssh_user }}/.ssh"
    private_key_source: "{{ ssh_dir }}/id_rsa"
    public_key_source: "{{ ssh_dir }}/id_rsa.pub"

  tasks:
    - name: Ensure .ssh directory exists
      file:
        path: "{{ ssh_dir }}"
        state: directory
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
        mode: '0700'

    - name: Copy shared private key
      copy:
        src: "{{ private_key_source }}"
        dest: "{{ ssh_dir }}/id_rsa"
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
        mode: '0600'

    - name: Copy shared public key
      copy:
        src: "{{ public_key_source }}"
        dest: "{{ ssh_dir }}/id_rsa.pub"
        owner: "{{ ssh_user }}"
        group: "{{ ssh_user }}"
        mode: '0644'

    - name: Add public key to authorized_keys
      authorized_key:
        user: "{{ ssh_user }}"
        key: "{{ lookup('file', public_key_source) }}"
        state: present
EOF

    chown ${ROOT_USER}:${ROOT_USER} "$PLAYBOOK_FILE"

    # Get all the IPs for names under the group [ssh]
    TARGET_GROUP="ssh"
    ALL_SSH_IPS=$(
        ansible-inventory -i "$INVENTORY_FILE" --list |
  	  jq -r --arg grp "$TARGET_GROUP" '
	    .[$grp].children[] as $child |
	    .[$child].hosts[]?'
    )

    # Do a ssh-keyscan and send it to every host to update known_hosts
    if [ -n "$ALL_SSH_IPS" ]; then
      echo "Scanning hosts to populate known_hosts to root..."
      echo "$ALL_SSH_IPS" | awk 'NF' | sort -u | xargs -r ssh-keyscan -H >> "$${ROOT_HOME}/.ssh/known_hosts"
    fi

    echo "Running Ansible playbook to distribute SSH keys to root..."
    sudo -u ${ROOT_USER} bash -c "ansible-playbook -i ${ROOT_HOME}/inventory.ini ${ROOT_HOME}/distribute_keys.yml"
    echo "Finished distributing SSH keys for root."
else
    echo "No clients or storage servers deployed or allow_root is not true"
fi

# --- ECGroup Configuration ---

if [ -n "$${ECGROUP_INSTANCES}" ]; then
    echo "Configuring ECGroup..."
    # Build the ECGroup ansible playbook
    cat <<EOF > /tmp/ecgroup.yml
- name: Configure ECGroup from the controller node
  hosts: all
  gather_facts: false
  vars:
    ecgroup_name: ecg
    ansible_user: admin
    ansible_ssh_private_key_file: ${PRIVATE_KEY_PATH}
    ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
  become: true
  tasks:
    - name: Create the cluster
      shell: >
        /opt/rozofs-installer/rozo_rozofs_create.sh -n {{ ecgroup_name }} -s "${ECGROUP_NODES}" -t external -d 3
      register: create_cluster_result

    - name: Add CTDB nodes
      shell: >
        /opt/rozofs-installer/rozo_rozofs_ctdb_node_add.sh -n {{ ecgroup_name }} -c "${ECGROUP_NODES}"
      register: ctdb_node_add_result

    - name: Setup DRBD
      shell: >
        /opt/rozofs-installer/rozo_drbd.sh -y -n {{ ecgroup_name }} -d "${ECGROUP_METADATA_ARRAY}"
      register: drbd_result

    - name: Create the array
      shell: >
        /opt/rozofs-installer/rozo_compute_cluster_balanced.sh -y -n {{ ecgroup_name }} -d "${ECGROUP_STORAGE_ARRAY}"
      register: compute_cluster_result

    - name: Propagate the configuration
      shell: >
        /opt/rozofs-installer/rozo_rozofs_install.sh -n {{ ecgroup_name }}
      register: install_result
  run_once: true
EOF

    # Wait for the instances to be ready
    PEERS=($ECGROUP_NODES)
    ALL=true
    for ip in "$${PEERS[@]}"; do
        echo "Waiting for $ip to open port 22..."
        SECONDS=0
        while ! nc -z -w1 "$ip" 22 &>/dev/null; do
            sleep 2
            if (( SECONDS >= 240 )); then
                echo "ERROR: $ip did not open port 22 after 240 seconds."
                ALL=false
                break
            fi
        done
    done

    if [ "$ALL" = true ]; then
        echo "All ECGroup instances are ready, provisioning!"
        sudo ansible-playbook /tmp/ecgroup.yml -i "$${ECGROUP_HOSTS},"
    else
        echo "Could not get all ECGroup instances in a ready state! Aborting configuration."
    fi
    echo "Finished ECGroup configuration."
else
    echo "No ECGroup deployed, skipping ECGroup configuration."
fi

# --- Hammerspace Anvil Configuration ---

if [ -n "$${MGMT_IP}" ] && \
   ( [ "$(echo "$${STORAGE_INSTANCES}" | jq length)" -gt 0 ] || \
     [ "$(wc -w <<< "$${ECGROUP_INSTANCES}")" -gt 0 ] ); then
    echo "Configuring Hammerspace Anvil..."
    cat > /tmp/anvil.yml << EOF
      data_cluster_mgmt_ip: "${MGMT_IP}"
      hsuser: admin
      password: "${ANVIL_ID}"
      volume_group_name: "${VG_NAME}"
      share_name: "${SHARE_NAME}"
EOF

    ECGROUP_NODES_ARRAY=($${ECGROUP_NODES})
    
    # Start the YAML file

    cat > /tmp/additional_ip_addresses.yml << 'EOF'
additional_ip_addresses:
  additionalAddresses: [
EOF

    # Add IP addresses starting from index 1 (skip first)

    for i in "$${!ECGROUP_NODES_ARRAY[@]}"; do
      if [ $${i} -gt 0 ]; then
        if [ $${i} -eq $(($${#ECGROUP_NODES_ARRAY[@]} - 1)) ]; then
          echo "        {ip: {address: \"$${ECGROUP_NODES_ARRAY[$i]}\", prefixLength: 32}}" >> /tmp/additional_ip_addresses.yml
        else
          echo "        {ip: {address: \"$${ECGROUP_NODES_ARRAY[$i]}\", prefixLength: 32}}," >> /tmp/additional_ip_addresses.yml
        fi
      fi
    done

    # Close the YAML structure

    echo "  ]" >> /tmp/additional_ip_addresses.yml

    echo "Created additional_ip_addresses.yml:"
    cat /tmp/additional_ip_addresses.yml

    NODE_SRC="$${ECGROUP_NODES:-$${STORAGE_INSTANCES}}"

    if [ -n "$NODE_SRC" ]; then
      if [ -n "$${ECGROUP_NODES}" ]; then
        FIRST_IP=$(echo "$NODE_SRC" | awk '{print $1}')
        cat > /tmp/nodes.yml <<EOF
    storages:
      - name: "ECGroup"
        nodeType: "OTHER"
        mgmtIpAddress:
          address: "$FIRST_IP"
        _type: "NODE"
EOF
    
      else
        printf '%s' "$NODE_SRC" | jq -r '
          "storages:",
          map(
            "- name: \"" + .name + "\"\n" +
            "  nodeType: \"OTHER\"\n" +
            "  mgmtIpAddress:\n" +
            "    address: \"" + .private_ip + "\"\n" +
            "  _type: \"NODE\""
          )[]
        ' > /tmp/nodes.yml
      fi
    fi

    printf '%s' 'share:
      name: "{{ share_name }}"
      path: "/{{ share_name }}"
      maxShareSize: 0
      alertThreshold: 90
      maxShareSizeType: TB
      smbAliases: []
      exportOptions:
      - subnet: "*"
        rootSquash: false
        accessPermissions: RW
      shareSnapshots: []
      shareObjectives:
      - objective:
          name: no-atime
        applicability: "TRUE"
      - objective:
          name: confine-to-{{ volume_group_name }}
        applicability: "TRUE"
      smbBrowsable: true
      shareSizeLimit: 0' > /tmp/share.yml

    sudo wget -O /tmp/hs-ansible.yml https://raw.githubusercontent.com/hammerspace-solutions/Terraform-AWS/main/modules/ansible/hs-ansible.yml
    sudo ansible-playbook /tmp/hs-ansible.yml -e @/tmp/anvil.yml -e @/tmp/nodes.yml -e @/tmp/share.yml -e @/tmp/additional_ip_addresses.yml
    echo "Finished Hammerspace Anvil configuration."
else
    echo "Either storage servers, ecgroup, or Anvil missing. Skipping Anvil configuration."
fi

echo "Ansible controller setup complete."
